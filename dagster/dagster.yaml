# Dagster Run Launcher
# --------------------
# The run launcher determines where runs are executed. 
# Reference: https://docs.dagster.io/deployment/dagster-instance#run-launcher

# Default: spawns a new process in the same node as a job's repository location.
run_launcher:
  module: dagster.core.launcher
  class: DefaultRunLauncher

# Allocates a Docker container per run. Spawns a new process in a docker container.
# run_launcher:
#   module: dagster_docker
#   class: DockerRunLauncher
  # env_vars:
  #     - DAGSTER_POSTGRES_HOSTNAME
  #     - DAGSTER_POSTGRES_USER
  #     - DAGSTER_POSTGRES_PASSWORD
  #     - DAGSTER_POSTGRES_DB
  # container_kwargs:
  #   volumes:
  #     # Save user repository code to volume 
  #     - dagster_capstone/repository.py:/opt/dagster/

scheduler:
  module: dagster.core.scheduler
  class: DagsterDaemonScheduler

# Run Coordinator
# ---------------
# The run coordinator determines the policy used to determine the 
# prioritization rules and concurrency limits for runs.  If you use 
# default coordinate, means immediately sends runs to the run launcher.
run_coordinator:
  module: dagster.core.run_coordinator
  class: QueuedRunCoordinator

# Kubernetes, allocates a Kubernetes Job per run. 
# run_launcher:
#   module: dagster_k8s.launcher
#   class: K8sRunLauncher
#   config:
#     service_account_name:
#       env: PIPELINE_RUN_SERVICE_ACCOUNT
#     job_image:
#       env: DAGSTER_IMAGE_NAME
#     instance_config_map:
#       env: DAGSTER_INSTANCE_CONFIG_MAP
#     postgres_password_secret:
#       env: DAGSTER_POSTGRES_SECRET


# Dagster Application Storage
# ---------------------------
# I have to create this storage.  I might use an RDS postgres as I am developing.
# Then I want to create a database is postgres.  So may have DEV environment (postgres)
# on my machine.  And PRODUCTION is in the cloud.
# Reference: https://docs.dagster.io/deployment/dagster-instance#schedule-storage
event_log_storage:
  module: dagster_postgres.event_log
  class: PostgresEventLogStorage
  config:
    postgres_db:
      username:
        env: POSTGRES_USER
      password:
        env: POSTGRES_PASSWORD
      hostname:
        env: POSTGRES_HOSTNAME
      db_name:
        env: POSTGRES_DB
      port: 5432


run_storage:
  module: dagster_postgres.run_storage
  class: PostgresRunStorage
  config:
    postgres_db:
      username:
        env: POSTGRES_USER
      password:
        env: POSTGRES_PASSWORD
      hostname:
        env: POSTGRES_HOSTNAME
      db_name:
        env: POSTGRES_DB
      port: 5432

schedule_storage:
  module: dagster_postgres.schedule_storage
  class: PostgresScheduleStorage
  config:
    postgres_db:
      username:
        env: POSTGRES_USER
      password:
        env: POSTGRES_PASSWORD
      hostname:
        env: POSTGRES_HOSTNAME
      db_name:
        env: POSTGRES_DB
      port: 5432

# # Store logs (Std output) in S3
# compute_logs:
#   module: dagster_aws.s3.compute_log_manager
#   class: S3ComputeLogManager
#   config:
#     bucket:
#       env: S3_BUCKET
#     prefix:
#       env: S3_LOG_PREFIX

# Store logs in directory of local machine or container
compute_logs:
  module: dagster.core.storage.local_compute_log_manager
  class: LocalComputeLogManager
  config:
    base_dir:
      env: LOCAL_COMPUTE_LOG_MANAGER_DIRECTORY

local_artifact_storage:
  module: dagster.core.storage.root
  class: LocalArtifactStorage
  config:
    base_dir: "/opt/dagster/local/"
  
# Allows opting out of Dagster collecting usage statistics.
# Reference: https://docs.dagster.io/deployment/dagster-instance#telemetry
telemetry:
  enabled: false